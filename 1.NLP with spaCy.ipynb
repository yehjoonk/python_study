{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6c9c06",
   "metadata": {},
   "source": [
    "# NLP with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e4732",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP) 자연어처리\n",
    "\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "컴퓨터를 이용해 사람의 자연어를 분석하고 처리하는 기술. 요소 기술로 자연어 분석, 이해, 생성 등이 있으며, 정보 검색, 기계 번역, 질의응답 등 다양한 분야에 응용된다.\n",
    "#### [Link to SpaCy documents](https://spacy.io/usage/linguistic-features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa217742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import explacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf11a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews_df=pd.read_csv('Reviews.csv')\n",
    "food_reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04402dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b91baa",
   "metadata": {},
   "source": [
    "### Tokenization 토큰화 \n",
    "First step in any NLP pipeline is tokenizing text i.e breaking down paragraphs into sentenses and then sentenses into words, punctuations and so on.\n",
    "\n",
    "* 말뭉치로부터 언어요소(Token)를 분리하는 작업\n",
    "\n",
    "\n",
    "###  Lemmatisation 표제어 처리\n",
    "Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "* 단어의 사전적 어원을 찾는 작업."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review=food_reviews_df.Text[54]\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a454e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en_core_web_sm')\n",
    "# creating a spacy object\n",
    "# https://spacy.io/models/en\n",
    "\n",
    "parsed_review = spacy_tok(sample_review)\n",
    "parsed_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "explacy.print_parse_info(spacy_tok, 'The salad was surprisingly tasty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explacy.print_parse_info(spacy_tok, food_reviews_df.Text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49030703",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) Tagging 품사 태깅\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "* 문장을 토큰화 한 후, 각 쪼개진 토큰에 영어의 8가지 품사 (명사, 동사, 대명사, 부사, 접속사, 전치사, 관사)를 부여하는 작업."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = pd.DataFrame()\n",
    "\n",
    "for i, token in enumerate(parsed_review):\n",
    "    tokenized_text.loc[i, 'text'] = token.text\n",
    "    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n",
    "    tokenized_text.loc[i, 'pos'] = token.pos_\n",
    "    tokenized_text.loc[i, 'tag'] = token.tag_\n",
    "    tokenized_text.loc[i, 'dep'] = token.dep_\n",
    "    tokenized_text.loc[i, 'shape'] = token.shape_\n",
    "    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n",
    "    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n",
    "    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n",
    "\n",
    "tokenized_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668edae",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) 개체명 인식\n",
    "Named entity is real world object like Person, Organization etc. \n",
    "* 사람, 장소, 기관, 날짜 등 분야별 각각 명명된 (named) 객체 텍스트를 식별화하는 작업."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(spacy_tok(food_reviews_df.Text[0]), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbe7ee",
   "metadata": {},
   "source": [
    "### Dependency parsing 의존구조 분석\n",
    "Syntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it. As in Subject combined with object makes a sentence. Spacy provides parse tree which can be used to generate this structure.\n",
    "\n",
    "* 수식을 받는 단어 (head / governor) -> 수식을 하는 단어 (dependent / modifier)들 사이의 의존관계를 파악하는 작업. \n",
    "\n",
    "#### Sentense Boundry Detection 문장 분리 작업\n",
    "Figuring out where sentense starts and ends is very imporatnt part of NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472088ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_spans = list(parsed_review.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da59fc",
   "metadata": {},
   "source": [
    "#### Visualising Dependency 의존 구조 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('det')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e01d09",
   "metadata": {},
   "source": [
    "#### Processing Noun Chunks 명사 청크 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c745724",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks_df = pd.DataFrame()\n",
    "\n",
    "for i, chunk in enumerate(parsed_review.noun_chunks):\n",
    "    noun_chunks_df.loc[i, 'text'] = chunk.text\n",
    "    noun_chunks_df.loc[i, 'root'] = chunk.root,\n",
    "    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n",
    "    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n",
    "    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n",
    "\n",
    "noun_chunks_df[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
